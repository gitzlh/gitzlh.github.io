---
layout:     post
title:      NLP炼金录-tirck篇
date:       2019-09-26
author:     zlh
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - NLP
    - 编程
---

#### 前言
主要包含提升模型效果的一些trick。

#### learning rate
- 对结果影响最大对超参数，调参应该把主要精力放在learning rate上。
- 对Transformer-based模型（比如BERT）, learning rate常常是是成与败的区别。
    某次实验中
    - 用Adam，lr设置为1e-3，loss停在一个较高值便不再下降。
    - 将lr改为1e-4，正常训练
    - lr = 1e-4，但将Adam改为SGD，loss直接变为nan。

#### Seq2Seq
- 共享embedding层和pre-softmax层对于提升效果很有帮助。


