---
layout:     post
title:      NLP炼金录
date:       2019-09-26
author:     zlh
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - NLP
    - 编程
---

## 前言

本文包含：

- 提升NLP模型指标的一些trick。
- 一些常见bug的原因及解决。

#### loss变为nan
- 用到了log函数，并且忘记平滑。
    ``` python
    >>> b = torch.tensor(1e-40)
    >>> torch.log(b)
    tensor(-92.1034)
    >>> b = torch.tensor(1e-50)
    >>> torch.log(b)
    tensor(-inf)
    ```

- 模型参数没有合理的初始化。
之前实现文本翻译LEAM模型对时候出现过。因为涉及到「取模」的操作，
未初始化导致模过大。
    ```python
    >>> a = torch.LongTensor(3,2)
    >>> a 
    tensor([[139938644376440,  94137537088864],
            [             32,              32],
            [              0, 139938644376440]])
    >>> a.norm()
    tensor(inf)

    ```


#### learning rate
- 对结果影响最大对超参数，调参应该把主要精力放在learning rate上。
- 对Transformer-based模型（比如BERT）, learning rate常常是是成与败的区别。
    某次实验中
    - 用Adam，lr设置为1e-3，loss停在一个较高值便不再下降。
    - 将lr改为1e-4，正常训练
    - lr = 1e-4，但将Adam改为SGD，loss直接变为nan。

#### BLEU
Github里十个脚本能测出十个结果...虽然结果差别都不大。
不过NLTK和NLG-Eval测出来的BLEU结果是完全一致的。

#### 词性标注
- NLTK ：很不准
- spacy：很不准
- Stanford NLP，java版：很不准
- Stanford NLP，python版本：很准

如，只有最后一个可以在lower case的情况下准确识别出NNP。

#### Seq2Seq
- 共享embedding层和pre-softmax层对于提升效果很有帮助。


#### 复现论文结果
- 一定要关注预处理，否则就差那几个点调不上去。
