---
layout:     post
title:      NLP炼金录
date:       2019-09-26
author:     zlh
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - NLP
    - 编程
---

## 前言

本文包含：

- 提升NLP模型指标的一些trick。
- 一些常见bug的原因及解决。

#### loss一段时间后变成nan

- 用到了log函数，并且忘记平滑。
    ``` python
    >>> b = torch.tensor(1e-40)
    >>> torch.log(b)
    tensor(-92.1034)
    >>> b = torch.tensor(1e-50)
    >>> torch.log(b)
    tensor(-inf)
    ```

- 模型参数没有合理的初始化。
之前实现文本翻译LEAM模型对时候出现过。因为涉及到「取模」的操作，
未初始化导致模过大。
    ```python
    >>> a = torch.LongTensor(3,2)
    >>> a 
    tensor([[139938644376440,  94137537088864],
            [             32,              32],
            [              0, 139938644376440]])
    >>> a.norm()
    tensor(inf)

    ```

#### 算loss时，ignore_index要设置正确：

- 有没有把PAD的值 ignore掉
- 词表里有没有值被错误地ignore掉？

#### BERT对与learning rate、optimizer的设置很敏感

某次实验中
- 用Adam，lr设置为1e-3，loss停在一个较高值便不再下降。
- 将lr改为1e-4，正常训练
- lr = 1e-4，但将Adam改为SGD，loss直接变为nan。


#### 学习率是对结果影响最大对超参数

#### BLEU等值等测量
Github里十个脚本能测出十个结果...虽然结果差别都不大。
不过NLTK和NLG-Eval测出来的BLEU结果是完全一致的。

#### 开源的词性标注工具
- NLTK ：很不准
- spacy：很不准
- Stanford NLP，java版：很不准
- Stanford NLP，python版本：很准

如只有最后一个可以在lower case的情况下准确识别出NNP。

#### Seq2Seq模型中，共享embedding层和pre-softmax层对于
提升效果很有帮助。
