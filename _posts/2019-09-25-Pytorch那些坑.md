---
layout:     post
title:      pytorch那些坑
date:       2019-09-25
author:     zlh
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - NLP
    - 编程
---

## 前言

用Pytorch搬砖也已经近一年了，写篇博客记录曾经、正在，以及今后在上面
睬的坑。

### model.eval() vs torch.no_grad()
虽然二者都是eval的时候使用，但其作用并不相同:
- model.eval() 负责改变batchnorm、dropout的工作方式，如在eval()模式下，dropout是不工作的。
见下方代码：
    ```python
    drop = nn.Dropout()
    x = torch.ones(10)
    # Train mode 
    drop.train()
    print(drop(x)) # tensor([2., 2., 0., 2., 2., 2., 2., 0., 0., 2.])
    # Eval mode
    drop.eval()
    print(drop(x)) # tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
    ```

- torch.no_grad() 负责关掉梯度计算，节省eval的时间。

个人理解是，model.eval()是必须使用的，否则会影响结果准确性。
而torch.no_grad()并不是强制的，只影响运行效率。

### LSTM 的 batch_first参数

`torch.nn.LSTM()` 里面的batch_first 参数只是针对input和output，不针对h,c.
即，即使指定了batch_first 参数，最终h，c输出的维度仍然是：
`(num_layers * num_directions, batch, hidden_size)`
