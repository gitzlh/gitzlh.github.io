---
layout:     post
title:      Pytorch那些坑
date:       2019-09-25
author:     zlh
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - NLP
    - 编程
---

## 前言

用Pytorch搬砖也已经近一年了，写篇博客记录曾经、正在，以及今后在上面
睬的坑。

### model.eval() vs torch.no_grad()

- model.eval() 负责改变batchnorm、dropout的工作方式，如在eval模式下，dropout是不工作的。
```
drop = nn.Dropout()
x = torch.ones(10)

# Train mode 
drop.train()
print(drop(x)) #tensor([2., 2., 0., 2., 2., 2., 2., 0., 0., 2.])

# Eval mode
drop.eval()
print(drop(x))  #tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
```
- torch.no_grad() 负责关掉梯度计算，节省eval的时间。

虽然二者都是eval的时候使用，但其作用并不相同。个人理解是，model.eval()是必须使用的，
而torch.no_grad()并不是强制的。


